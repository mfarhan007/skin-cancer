{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.5.1\n",
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tempfile\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from tensorflow.keras.applications import imagenet_utils,ResNet50,VGG16\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Dense,GlobalAveragePooling2D,concatenate\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import cv2\n",
    "print(cv2.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H:\\ISIC Data\\jpeg\\train\\benign\n",
      "H:\\ISIC Data\\jpeg\\train\\malig\n",
      "H:\\ISIC Data\\jpeg\\val\\benign\n",
      "H:\\ISIC Data\\jpeg\\val\\malig\n"
     ]
    }
   ],
   "source": [
    "base_dir = 'H:\\ISIC Data\\jpeg\\\\'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'val')\n",
    "\n",
    "# Directory with training benign pictures\n",
    "train_benign_dir = os.path.join(train_dir, 'benign')\n",
    "\n",
    "# Directory with training malignant pictures\n",
    "train_mal_dir = os.path.join(train_dir, 'malig')\n",
    "\n",
    "# Directory with validation benign pictures\n",
    "validation_benign_dir = os.path.join(validation_dir, 'benign')\n",
    "\n",
    "# Directory with validation malignant pictures\n",
    "validation_mal_dir = os.path.join(validation_dir, 'malig')\n",
    "\n",
    "print(train_benign_dir)\n",
    "print(train_mal_dir)\n",
    "print(validation_benign_dir)\n",
    "print(validation_mal_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_tr = os.listdir(train_benign_dir)\n",
    "mal_tr = os.listdir(train_mal_dir)\n",
    "\n",
    "benign_val = os.listdir(validation_benign_dir)\n",
    "mal_val = os.listdir(validation_mal_dir)\n",
    "\n",
    "benign_tr = [os.path.join(train_benign_dir, x) for x in benign_tr]\n",
    "mal_tr = [os.path.join(train_mal_dir, x) for x in mal_tr]\n",
    "benign_val = [os.path.join(validation_benign_dir, x) for x in benign_val]\n",
    "mal_val = [os.path.join(validation_mal_dir, x) for x in mal_val]\n",
    "\n",
    "total_train = benign_tr + mal_tr\n",
    "total_val = benign_val + mal_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H:\\\\ISIC Data\\\\jpeg\\\\val\\\\benign\\\\ISIC_2793142.jpg'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_array(total):\n",
    "    random.shuffle(total)\n",
    "    X = np.zeros(shape=(len(total),224,224,3), dtype=np.uint8)\n",
    "    y = []\n",
    "    for i in range(len(total)):\n",
    "        img = cv2.imread(total[i])\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "        X[i] = img\n",
    "        if len(re.findall('benign', total[i])) == 1:\n",
    "            y.append(0)\n",
    "        else:\n",
    "            y.append(1)\n",
    "    y = np.array(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = data_to_array(total_train)\n",
    "X_test, y_test = data_to_array(total_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27026 (27026, 224, 224, 3)\n",
      "27026 (27026,)\n",
      "6100 (6100, 224, 224, 3)\n",
      "6100 (6100,)\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train) , X_train.shape )\n",
    "print(len(y_train) , y_train.shape )\n",
    "print(len(X_test) , X_test.shape )\n",
    "print(len(y_test) , y_test.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train = np.concatenate((y_train, [1]))\n",
    "#y_test = np.concatenate((y_test, [1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(base_model):\n",
    "    base_model.trainable = False\n",
    "    global_average_layer = GlobalAveragePooling2D()(base_model.output)\n",
    "    prediction_layer = Dense(1, activation='sigmoid')(global_average_layer)\n",
    "    model = Model(inputs=base_model.input, outputs=prediction_layer)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_loading stuff\n",
    "class FixedDropout(tf.keras.layers.Dropout):\n",
    "    def _get_noise_shape(self, inputs):\n",
    "        if self.noise_shape is None:\n",
    "            return self.noise_shape\n",
    "\n",
    "        symbolic_shape = K.shape(inputs)\n",
    "        noise_shape = [symbolic_shape[axis] if shape is None else shape\n",
    "                       for axis, shape in enumerate(self.noise_shape)]\n",
    "        return tuple(noise_shape)\n",
    "def load_all_models():\n",
    "    all_models = []\n",
    "    model_names = ['vgg_model.h5', 'resnet_model.h5', 'eff_model.h5']\n",
    "    for model_name in model_names:\n",
    "        filename = os.path.join('H:/ISIC Data/notebooks/models/', model_name)\n",
    "        model = tf.keras.models.load_model(filename,custom_objects={'FixedDropout':FixedDropout})\n",
    "        all_models.append(model)\n",
    "        print('loaded:', filename)\n",
    "    return all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stacked model from multiple member input models\n",
    "\n",
    "def define_stacked_model(models,output_bias = None):\n",
    "    for i,model in enumerate(models):\n",
    "        for layer in model.layers:\n",
    "            layer.trainable = False\n",
    "            layer._name = 'ensemble_' + str(i+1) + '_' + layer.name\n",
    "    ensemble_visible = [model.input for model in models]\n",
    "    ensemble_outputs = [model.output for model in models]\n",
    "    merge = concatenate(ensemble_outputs)\n",
    "    hidden = Dense(10, activation='relu')(merge)\n",
    "    output = Dense(1, activation='sigmoid',bias_initializer=output_bias)(hidden)\n",
    "    model = Model(inputs=ensemble_visible, outputs=output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=tf.keras.metrics.AUC(name='auc'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded: H:/ISIC Data/notebooks/models/vgg_model.h5\n",
      "loaded: H:/ISIC Data/notebooks/models/resnet_model.h5\n",
      "loaded: H:/ISIC Data/notebooks/models/eff_model.h5\n"
     ]
    }
   ],
   "source": [
    "models = load_all_models()\n",
    "model = define_stacked_model(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create input lists as we have to input to multiple input heads for multiple models\n",
    "X = [X_train for _ in range(len(model.input))]\n",
    "X_1 = [X_test for _ in range(len(model.input))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "422/422 [==============================] - 4090s 10s/step - loss: 0.4948 - auc: 0.5010 - val_loss: 0.1586 - val_auc: 0.4751\n",
      "Epoch 2/5\n",
      "422/422 [==============================] - 4069s 10s/step - loss: 0.1310 - auc: 0.4939 - val_loss: 0.0915 - val_auc: 0.4828\n",
      "Epoch 3/5\n",
      "422/422 [==============================] - 4085s 10s/step - loss: 0.0911 - auc: 0.4823 - val_loss: 0.0850 - val_auc: 0.4895\n",
      "Epoch 4/5\n",
      "422/422 [==============================] - 4043s 10s/step - loss: 0.0924 - auc: 0.4915 - val_loss: 0.0841 - val_auc: 0.4908\n",
      "Epoch 5/5\n",
      "422/422 [==============================] - ETA: 0s - loss: 0.0920 - auc: 0.4962"
     ]
    }
   ],
   "source": [
    "# fit stacked model on test dataset\n",
    "BATCH_SIZE = 64\n",
    "stacked_model_history = model.fit(X, y_train,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    steps_per_epoch=len(total_train) // BATCH_SIZE,\n",
    "                    epochs=5,\n",
    "                    validation_data=(X_1, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = stacked_model_history.history['auc']\n",
    "val_auc = stacked_model_history.history['val_auc']\n",
    "\n",
    "loss = stacked_model_history.history['loss']\n",
    "val_loss = stacked_model_history.history['val_loss']\n",
    "\n",
    "epochs_range = range(5)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, auc, label='Training AUC')\n",
    "plt.plot(epochs_range, val_auc, label='Validation AUC')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Testing AUC')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Testing Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
